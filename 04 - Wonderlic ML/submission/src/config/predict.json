{
  "merger_params": {
    "predictions_folder": "results/test",
    "merged_prediction_name": "test_prediction.csv"
  },
  "benchmarks": {
    "empathy": {
      "model_path": "wonderlic-engineering/sift-bart-large-mnli-empathy",
      "test_data_path": "data/empathy/empathy_test.csv",
      "predictions_path": "results/test"
    },
    "interview": {
      "model_params": {
        "model": "gpt-4-turbo-preview",
        "temperature": 1,
        "max_tokens": 200,
        "seed": 12345
      },
      "message_params": {
        "system_message": "You are a job candidate and you are responding to behavioral questions during an interview.",
        "task_explanation": "You will be provided with questions and answers you already answered in between ### markers. You will need to answer the last question in a consistent style, using less than five sentences."
      },
      "file_paths": {
        "interview_fp": "data/interview/interview_test.csv",
        "personality_fp": "data/interview/personality_test.csv",
        "interpretation_fp": "data/interview/personality.json",
        "results_fp": "results/test"
      },
      "secret_key": "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"
    },
    "fairness": {
      "model_params": {
        "model": "gpt-4-turbo-preview",
        "temperature": 0,
        "max_tokens": 1,
        "seed": 424
      },
      "message_params": {
        "system_message": "You are an Industrial-Organizational Psychologist.",
        "task_explanation": "Respondents compared two organizational policies and voted on which was fairest. Identify which policy received the majority vote as the fairer option. Answer only with 'first' or 'second', even in uncertainty. 'Both' as a response is not valid."
      },
      "file_paths": {
        "train_data_fp": "data/fairness/fairness_train.csv",
        "test_data_fp": "data/fairness/fairness_test.csv",
        "predictions_path": "results/test"
      },
      "secret_key": "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"
    },
    "clarity": {
      "device": "cuda",
      "models_params_dict": {
        "model1": {
        "peft_id": "wonderlic-engineering/bloomz-1b1-prompt-tuned-siop-clarity-1k",
        "model_id": "bigscience/bloomz-1b1",
        "eos_token": 2,
        "max_tokens": 10,
          "max_length": 70
        },
      "model2": {
        "peft_id": "wonderlic-engineering/gpt2-medium-prompt-tuned-siop-clarity-1k",
        "model_id": "openai-community/gpt2-medium",
        "eos_token": 50256,
        "max_tokens": 11,
        "max_length": 70
      }
      },
      "question": "What is the average clarity rating on a scale of 1-7 of the personality item?",
      "file_paths": {
        "test_data_fp": "data/clarity/clarity_test.csv",
        "predictions_path": "results/test"
      }
    }
  }
}